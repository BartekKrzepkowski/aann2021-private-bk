{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was inspired by neural network & machine learning labs led by [GMUM](https://gmum.net/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also [An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/index.html), section 8.3 of [Chapter 8](https://www.deeplearningbook.org/contents/optimization.html) of the Deep Learning book, [Why Momentum Really Works](https://distill.pub/2017/momentum/), and the [second video](https://www.youtube.com/watch?v=IHZwWFHWa-w) from the 3Blue1Brown playlist on neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils import visualize_optimizer, test_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "Now that we’ve seen how to compute derivatives of the cost function with\n",
    "respect to model parameters last class, what do we do with those derivatives? \n",
    "\n",
    "Today we'll talk about several popular optimization methods. One thing that's perhaps important to note before we get started -- in mathematical optimization in general, the only thing we care about is optimizing some function we know. This is not usually the case in deep learning (or machine learning in general) -- we have access to a measure of performance on the training set, but what we really care about is generalization.\n",
    " \n",
    "\n",
    "[]()                       |  []()\n",
    ":-------------------------:|:-------------------------:\n",
    "![](figures/contours_evaluation_optimizers.gif)  |  ![](figures/saddle_point_evaluation_optimizers.gif)\n",
    "\n",
    "<center>Visualization of several popular optimizers. Source: <a href=\"https://ruder.io/optimizing-gradient-descent/index.html\">An overview of gradient descent optimization algorithms</a>.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task today will be to implement some of the most popular optimizers in deep learning. Each of the classes you'll be implementing inherits from the `Optimizer` class, which already has some needed functionality. You'll need to overload the `step` method and create some variables in the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"Base class for each optimizer\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_params):\n",
    "        # store initial model weights\n",
    "        self.params = initial_params\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"Updates the weights stored in self.params\"\"\"\n",
    "        raise NotImplementedError()\n",
    "         \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Torch accumulates gradients, so we need to clear them after every update\"\"\"\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.detach_()\n",
    "                param.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: SGD\n",
    "SGD has one hyperparameter $\\eta$, which corresponds to the learning rate: $\\mathbf{\\theta_{new}}=\\mathbf{\\theta_{old}} -\\eta \\mathbf{g}$, where $\\mathbf{g}=\\nabla_\\mathbf{\\theta} L(\\mathbf{\\theta})$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class GradientDescent(Optimizer):\n",
    "    \n",
    "    def __init__(self, initial_params: List[torch.tensor], learning_rate):\n",
    "        super().__init__(initial_params)\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param -= self.learning_rate * param.grad\n",
    "            # Please note that it's important to change the parameters in-place (-=) so the original tensors are modified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will tell you whether the optimizer works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_optimizer(GradientDescent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will visualize the optimization trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_optimizer(GradientDescent, n_steps=20, learning_rate=0.1, title='Small LR')\n",
    "visualize_optimizer(GradientDescent, n_steps=15, learning_rate=0.5, title='Large LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (0.25p): Momentum\n",
    "As we can see above, SGD can have trouble navigating ravines -- areas where the surface curves much more steeply in one dimension than in another. In such scenarios, SGD oscillates across the slopes of the ravine while only making meagre progress towards the local optimum. Momentum is a method to dampen these oscillations by introducing memory into the update rule (think of a ball rolling down a hill):\n",
    "\n",
    "$$\\mathbf{v_{new}} = \\gamma \\mathbf{v_{old}} + \\eta \\mathbf{g},$$\n",
    "\n",
    "$$\\mathbf{\\theta_{new}}=\\mathbf{\\theta_{old}}-\\mathbf{v_{new}}.$$\n",
    "\n",
    "The momentum $\\gamma$ is usually set to $0.9$ (always less than $1$), whereas the learning rate $\\eta$ usually ranges from $0.001$ to $0.01$. The initial $\\mathbf{v}$ should be initialized to zeroes (one $\\mathbf{v}$ for each parameter with the same shape as that parameter; use `torch.zeros_like`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum(Optimizer):\n",
    "    \n",
    "    def __init__(self, initial_params, learning_rate, gamma):\n",
    "        super().__init__(initial_params)\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        ???\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        # hint: zip is useful if you want to iterate over two lists at once\n",
    "        ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_optimizer(Momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_optimizer(Momentum, n_steps=20, learning_rate=0.05, gamma=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (0.25p): Adagrad\n",
    "One problem with the above methods is that the same learning rate applies to all parameter updates. If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent (we'd like to perform smaller updates to features ocurring more frequently and larger updates to rare features). Adagrad fixes this by using a different learning rate for every parameter based on the sum of squares of the gradients so far:\n",
    "\n",
    "$$\\mathbf{r_{new}} = \\mathbf{r_{old}} + \\mathbf{g}^2,$$\n",
    "\n",
    "$$\\mathbf{\\theta_{new}}=\\mathbf{\\theta_{old}}-\\frac{\\eta}{\\sqrt{\\mathbf{r_{new}+\\epsilon}}}\\cdot \\mathbf{g},$$\n",
    "where the relevant multiplications are done element-wise.\n",
    "\n",
    "Adagrad has two hyperparameters:\n",
    "- the global learning rate $\\eta$, typically from $0.001$ to $0.01$,\n",
    "- $\\epsilon$ -- used to prevent division by $0$, typically set to $10^{-8}$.\n",
    "\n",
    "The initial $\\mathbf{r}$ should be set to a tensor of zeroes with the same shape as a given parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad(Optimizer):\n",
    "    \n",
    "    def __init__(self, initial_params, learning_rate, epsilon):\n",
    "        super().__init__(initial_params)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        ???\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_optimizer(Adagrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_optimizer(Adagrad, n_steps=20, learning_rate=1.0, epsilon=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 (0.25p): RMSProp\n",
    "Adagrad's main weakness is its accumulation of the squared gradients in the denominator. As we keep on adding positive terms, the sum keeps growing during training. This, in turn, can result in a premature and excessive decrease in the eﬀective learning rate. RMSProp and Adadelta (another method which we won't talk about here) seek to alleviate this problem, which RMSProp does by using an exponentially decaying average to discard history from the extreme past:\n",
    "\n",
    "$$\\mathbf{r_{new}}=\\gamma\\mathbf{r_{old}}+(1-\\gamma)\\mathbf{g}^2,$$\n",
    "\n",
    "$$\\mathbf{\\theta_{new}}=\\mathbf{\\theta_{old}}-\\frac{\\eta}{\\sqrt{\\mathbf{r_{new}+\\epsilon}}}\\cdot \\mathbf{g}.$$\n",
    "\n",
    "RMSProp has three hyperparameters:\n",
    "- the global learning rate $\\eta$, typically set from $0.001$ to $0.01$,\n",
    "- the moving average coefficient $\\gamma$, typically set to $0.9$,\n",
    "- $\\epsilon$ -- used to prevent division by $0$, typically set to $10^{-8}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp(Optimizer):\n",
    "    \n",
    "    def __init__(self, initial_params, learning_rate, gamma, epsilon):\n",
    "        super().__init__(initial_params)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        ???\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_optimizer(RMSProp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_optimizer(RMSProp, n_steps=10, learning_rate=0.5, gamma=0.9, epsilon=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (0.5p): Adam\n",
    "Adam (cf. _adaptive moments_) is yet another adaptive learning rate optimization algorithm.  In addition to storing an exponentially decaying average of past squared gradients like RMSprop, Adam also keeps an exponentially decaying average of past gradients, similar to momentum. We compute the decaying averages of past and past squared gradients $\\mathbf{m}$ and $\\mathbf{v}$ respectively as follows:\n",
    "\n",
    "$$\\mathbf{m_{new}}=\\beta_1\\mathbf{m_{old}}+(1-\\beta_1)\\mathbf{g},$$\n",
    "\n",
    "$$\\mathbf{v_{new}}=\\beta_2\\mathbf{v_{old}}+(1-\\beta_2)\\mathbf{g}^2.$$\n",
    "\n",
    "These are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, hence the name of the method. As $\\mathbf{m}$ and $\\mathbf{v}$ are initialized as vectors of zeroes, these estimates are initially biased towards zero, hence we correct these updates:\n",
    "\n",
    "$$\\hat{\\mathbf{m}}_{\\mathbf{new}} = \\frac{\\mathbf{m_{new}}}{1-\\beta_1^t},$$\n",
    "$$\\hat{\\mathbf{v}}_{\\mathbf{new}} = \\frac{\\mathbf{v_{new}}}{1-\\beta_2^t},$$\n",
    "\n",
    "where $t$ is the current timestep, i.e. $\\mathbf{m_{new}}=\\mathbf{m_t}$. Then we use these for the actual update:\n",
    "\n",
    "$$\\mathbf{\\theta_{new}}=\\mathbf{\\theta_{old}}-\\frac{\\eta}{\\sqrt{\\hat{\\mathbf{v}}_{\\mathbf{new}}}+\\epsilon}\\cdot \\hat{\\mathbf{m}}_{\\mathbf{new}}.$$\n",
    "\n",
    "\n",
    "Adam has four hyperparameters:\n",
    "- the global learning rate $\\eta$, typically set to $0.001$,\n",
    "* the moving average coefficient of the first moment $\\beta_1$, typically 0.9,\n",
    "* the moving average coefficient of the second moment $\\beta_2$, typically 0.999,\n",
    "- $\\epsilon$ -- used to prevent division by $0$, typically set to $10^{-8}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "   \n",
    "    def __init__(self, initial_params, learning_rate, beta1, beta2, epsilon):\n",
    "        super().__init__(initial_params)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        ???\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_optimizer(Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_optimizer(Adam, n_steps=60, learning_rate=0.5, beta1=0.9, beta2=0.999, epsilon=1e-8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
