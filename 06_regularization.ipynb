{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was inspired by neural network & machine learning labs led by [GMUM](https://gmum.net/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also [How does Batch Normalization Help Optimization?](https://gradientscience.org/batchnorm/) and [Chapter 7](https://www.deeplearningbook.org/contents/regularization.html) of the Deep Learning book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utils for today's class (run and hide the cell):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, Lambda, ToTensor\n",
    "\n",
    "from utils import load_fashionmnist, ModelTrainer, show_results, test_dropout, test_bn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization \n",
    "One of the main problems in machine learning is what happens when we run our model on new inputs. There are a lot of techniques and strategies designed to reduce test error, even at the expense of training error. Today we'll be talking about some of them. We'll be working (once again) with the FashionMNIST dataset. The cell below loads in the datasets. If the networks train too slowly, you can play with the `shrinkage` parameter, which determines how much of the dataset is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(44)\n",
    "\n",
    "train_dataset = load_fashionmnist(train=True, shrinkage=0.01)\n",
    "test_dataset = load_fashionmnist(train=False, shrinkage=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below sets some hyperparameters for all of the models trained in the notebook. They should be set such that for all the models the learning curve flattens. The chosen hyperparameters should work, but the training might be a bit slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 300\n",
    "learning_rate = 0.05\n",
    "batch_size = 128\n",
    "\n",
    "trainer = ModelTrainer(train_dataset, test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (0.25p)\n",
    "Use [`torch.nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) to create a simple neural network. It should have two hidden linear layers of size $256$ and ReLU activation functions and an output layer with the linear activation function (i.e. none). This network will serve as a baseline for today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    ???\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "history = trainer.train(model, optimizer, n_epochs=n_epochs)\n",
    "show_results(model=history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above was defined correctly, you can see that after ~$50$ epochs (if you used the same hyperparameters as suggested above) the test loss starts rising and the test accuracy flattens out. This means that our model has started *overfitting* to the training set. If we want to get the best model out of this architecture, we would need to load in the parameters from the moment when the test accuracy was highest (so-called *early stoppping*, perhaps the most popular regularization technique in deep learning). Your task today will be to improve upon these results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Norm Penalties\n",
    "A simple way of doing non-architecture-dependent regularization is by adding a penalty $\\Omega(\\theta)$ to the loss function $J(\\theta)$:\n",
    "\n",
    "$$\\tilde{J}(\\theta) = J(\\theta) + \\alpha \\Omega(\\theta),$$\n",
    "\n",
    "where $\\alpha \\in [0, \\infty)$ is a hyperparameter that weights the contribution of the penalty term $\\Omega$.\n",
    "\n",
    "When using such regularization for neural networks, we typically penalize only the weights and not the biases -- we do not introduce too much variance by leaving them unregularized and doing so can introduce significant underfitting.\n",
    "\n",
    "$L_1$ and $L_2$ regularization, which you might know from regular machine learning, fall into this family of methods. We will not be talking about them today, but move to methods specific to neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (0.75p): Dropout\n",
    "Dropout is regularization method where during training some number of outputs are randomly ignored (or *dropped out*). This prevents complex co-adaptations from arising (e.g. one neuron learning to fix the mistakes of another), which makes the model more robust.\n",
    "\n",
    "![dropout](figures/dropout.png)\n",
    "<center>Source: <a href=\"https://jmlr.org/papers/v15/srivastava14a.html\">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a>.</center>\n",
    "\n",
    "Dropout is not used after testing, but we need to correct for the fact that the network is used to smaller outputs, hence we need to scale the output by the chosen dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, p=0.5):\n",
    "        super(Dropout, self).__init__()\n",
    "        self.p = p\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            # hint: use torch.bernoulli\n",
    "            ???            \n",
    "        else:\n",
    "            ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dropout(Dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add dropout with probability $0.5$ to the baseline model after each hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    ???\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "dropout_history = trainer.train(model, optimizer, n_epochs=n_epochs)\n",
    "show_results(model=dropout_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "1. How does the testing accuracy curve after applying dropout look in comparison to the baseline model? What does that suggest? \n",
    "2. How can we think of dropout as ensemble learning (combining many models at once)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your answers here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 (1p): Batch Normalization\n",
    "\n",
    "Batch Normalization was introduced as a technique to reduce *internal covariate shift*. To understand this perspective, recall that training a neural network can be viewed as solving a collection of separate optimization problems -- one for each layer:\n",
    "\n",
    "![layer based](figures/layerbased.jpg)\n",
    "<center>Source: <a href=\"https://gradientscience.org/batchnorm/\">How does Batch Normalization Help Optimization?</a></center>\n",
    "\n",
    "During training, each step involves updating the parameters for all layers simultaneously. This implies that updates to the earlier layers change the input distribution of later layers, hence the optimization problems change at each step (this is the aforementioned internal covariate shift). To fix that, we whiten the input, so the input distribution is the same:\n",
    "\n",
    "$$\\hat{\\mathbf{x}_i} = \\frac{\\mathbf{x}_i - \\mathbf{\\mu_B}}{\\sqrt{\\mathbf{\\sigma^2_B} + \\epsilon}},$$\n",
    "\n",
    "where $\\mathbf{\\mu_B}=\\frac{1}{m}\\sum_{i=1}^m \\mathbf{x}_i$ and $\\mathbf{\\sigma^2_B}  = \\frac{1}{m} \\sum_{i=1}^m (\\mathbf{x}_i - \\mathbf{\\mu_B)^2}$ are the batch mean and batch variance respectively (ideally we'd want to do this over the whole training set, but in the context of stochastic gradient methods that would be impractical) and $\\epsilon$ is added for numerical stability. To restore the representational power of the network, we modify $\\hat{\\mathbf{x}_i}$ with learned parameters $\\gamma$ and $\\beta$, so any mean and variance can be learned. In the end, the batch norm layer looks like this:\n",
    "\n",
    "$$\\mathtt{BN}(\\mathbf{x}_i) = \\gamma \\hat{\\mathbf{x}_i} + \\beta.$$\n",
    "\n",
    "During testing, we replace the batch statistics with population statistics computed during training via running means:\n",
    "\n",
    "$$\\mathbf{\\overline{\\mu}_{new}} =  (1 - \\lambda) \\mathbf{\\overline{\\mu}_{old}} + \\lambda \\mathbf{\\mu_B},$$\n",
    "\n",
    "$$\\mathbf{\\overline{\\sigma}_{new}} = (1 - \\lambda) \\mathbf{\\overline{\\sigma}_{old}} + \\lambda \\mathbf{\\sigma_B},$$\n",
    "\n",
    "where $\\lambda$ is the momentum term for the running means.\n",
    "\n",
    "While the effectiveness of batch normalization is hard to dispute, the proposed mechanism is contested. See [How does Batch Normalization Help Optimization?](https://gradientscience.org/batchnorm/) for more on this topic.\n",
    "\n",
    "When defining model parameters it can be useful to utilize [`torch.Parameter`](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html). When defining non-trainable parameters (e.g. running means) use [`torch.nn.Module.register_buffer`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features, eps=1e-05, momentum=0.1):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        ???\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            ???\n",
    "            \n",
    "        else:\n",
    "            ???\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bn(BatchNorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add batch normalization to the baseline model after each hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    ???\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "bn_history = trainer.train(model, optimizer, n_epochs=n_epochs)\n",
    "show_results(model=bn_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "1. Should batch normalization be used before or after the activation function? Why? (Hint: not sure there is a good answer.)\n",
    "2. Can we think of batch normalization as regularization? Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your answers here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(vanilla=history, dropout=dropout_history, bn=bn_history, \n",
    "             orientation='vertical', accuracy_bottom=0.5, loss_top=1.75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
