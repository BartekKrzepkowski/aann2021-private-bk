{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was inspired by neural network & machine learning labs led by [GMUM](https://gmum.net/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional sources regarding today's material, I recommend reading the beginning of [Chapter 6](https://www.deeplearningbook.org/contents/mlp.html) of the Deep Learning book, especially up to and including 6.1 (*Example: Learning XOR*). Much of the following section follows that chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks\n",
    "\n",
    "![venn diagram](figures/fig2.png)\n",
    "<center>Source: <a href=\"https://www.deeplearningbook.org/contents/intro.html\">Chapter 1</a> of the Deep Learning book.</center>\n",
    "\n",
    "Neural networks are a type of universal function approximator loosely based on biological systems.\n",
    "\n",
    "The goal of a neural network is to approximate some function $f^*$. For example, in the case of a classifier, $f^*(\\mathbf{x})=y$ maps an input $x$ to some category $y$. A neural network defines a mapping $f(\\mathbf{x};\\theta)=y$ and learns the value of the parameters $\\theta$ which best approximate the function $f^*$.\n",
    "\n",
    "Neural networks are *hierarchical*, in the sense that they are typically represented by composing together many diﬀerent functions. For example, $f(\\mathbf{x})=f^{(3)}(f^{(2)}(f^{(1)}(\\mathbf{x})))$ is a function comprised of three different functions composed together. In this case, $f^{(1)}$ is called the *first layer* of the network, $f^{(2)}$ the *second layer*, and so on. The length of this chain of functions is called the *depth* of the model (this is where the term *deep learning* comes from). The final layer of the network is called the *output layer*.\n",
    "\n",
    "During training, we want $f$ to match $f^*$, which implies that the output of the last layer is somewhat determined. That is not the case for the other layers, though -- the learning algorithm must decide how to use those layers to produce the desired output, but the training data do not say what each individual layer should do. As the training data does not show the desired output for each of these layers, they are called *hidden layers*.\n",
    "\n",
    "Each hidden layer of the network is typically vector valued. The dimensionality of these hidden layers determines the *width* of the model. Each element of the vector may be interpreted as playing a role analogous to a neuron.\n",
    "<center> <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/1200px-Colored_neural_network.svg.png\" width=300 /> \n",
    "A neural network with one hidden layer comprised of four neurons. Source: <a href=\"https://en.wikipedia.org/wiki/Artificial_neural_network\">Wikipedia</a>. </center>\n",
    "\n",
    "Instead of thinking of the layer as representing a single vector-to-vector function, we can also think of the layer as consisting of many *units* that act in parallel, each representing a vector-to-scalar function. The choice of the functions used to compute these representations is also loosely guided by neuroscientiﬁc observations about the functions that biological neurons compute. Each unit resembles a neuron in the sense that it receives input from many other units and computes its own activation value. \n",
    "\n",
    "In modern deep learning, the standard layer can be written as $g(\\Theta^T\\mathbf{x}+\\mathbf{b})$, where $\\mathbf{x}$ is the output of the previous layer, $\\Theta$ is the parameter (or weight) matrix for the given layer, $\\mathbf{b}$ is the bias vector, and $g$ is an activation function (e.g. the sigmoid). In this terminology, we can write the $j$-th neuron of that layer as $g(\\theta_j^T\\mathbf{x}+\\mathbf{b}_j)$, where $\\theta_j$ is the $j$-th column of $\\Theta$ (or the $j$-th row of $\\Theta^T$), $\\mathbf{b}_j$ is the $j$-th value of the bias vector, and $g$ is the aforementioned activation function. We can write the whole neural network as $$f(x)=g_D(\\Theta_D^T( g_{D-1}(\\Theta_{D-1}^T(\\ldots g_1(\\Theta_1^T\\mathbf{x}+\\mathbf{b}_1)\\ldots)+\\mathbf{b}_{D-1}) +\\mathbf{b}_D),$$ where $g_i$, $\\Theta_i$, and $\\mathbf{b}_i$ are the activation function, parameter matrix, and bias vector for the $i$-th layer, respectively, and $D$ is the depth of the network.\n",
    "\n",
    "To better understand neural networks, let's look at the linear model from last week (multinomial logistic regression): $$f(x;\\theta)=\\mathtt{softmax}(\\Theta^T\\mathbf{x}+\\mathbf{b}).$$\n",
    "\n",
    "We can think of it as a simple neural network with only one hidden layer, with the activation function being the $\\mathtt{softmax}$. \n",
    "<img src=\"https://i.kym-cdn.com/photos/images/newsfeed/000/531/557/a88.jpg\" /> \n",
    "<center> Why not stop at single-layer neural networks? </center>\n",
    "\n",
    "Linear models are appealing, because they can be fit efficiently and reliably. Linear models also have the obvious defect that their model capacity is limited to linear functions, so the model cannot understand more complicated interactions between the input variables.\n",
    "\n",
    "Another way to look at this is that in order to tackle nonlinear problems, we need to somehow transform the data to a space where the problem is linear. This can be done in a variety of manners, such as via kernel methods (e.g. support vector machines) or specifying the transformation manually (this was the dominant approach until the advent of deep learning, with different methods for different tasks, and practitioners specializing in diﬀerent domains, such as speech recognition or computer vision, and with little transfer between domains). \n",
    "\n",
    "![representation](figures/fig1.png)\n",
    "<center>Source: <a href=\"https://www.deeplearningbook.org/contents/intro.html\">Chapter 1</a> of the Deep Learning book.</center>\n",
    "\n",
    "In the case of deep learning, we simply learn the new representation!\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/img/spiral.1-2.2-2-2-2-2-2.gif\" width=300 />\n",
    "<center>Source: <a href=\"https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/\">Neural Networks, Manifolds, and Topology</a>.</center>\n",
    "\n",
    "How does this look in the context of an actual (convolutional) neural network (more on this later)?\n",
    "![representation](figures/fig3.png)\n",
    "<center>Source: <a href=\"https://www.deeplearningbook.org/contents/intro.html\">Chapter 1</a> of the Deep Learning book.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (1p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TensorFlow's [Playground](http://playground.tensorflow.org/) answer the following questions. Some remarks:\n",
    "- Each answer should be 2 sentences max. \n",
    "- When specifying the architecture of a network, it's enough to write it out as $n_1$-$n_2$-$\\ldots$-$n_k$, where $n_i$ is the number of neurons in the $i$-th layer and $k$ is the number of layers (e.g. 5-3-6 specifies a neural network with 5 neurons in the first layer, 3 neurons in the second layer, and 6 neurons in the third layer).\n",
    "- Don't change the amount of noise or the ratio of train to test data.\n",
    "- Don't change the input features unless instructed so in the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gauss dataset\n",
    "\n",
    "- Is this dataset a priori solvable by *shallow* methods?\n",
    "> Yes, no hidden layer is needed to disciminate between these two clouds of points\n",
    "> <img src=\"gauss_shallow.png\" width=\"800\" height=\"400\">\n",
    "- What makes this dataset easier than the others?\n",
    "> This dataset is consists of two linearly-separated points clouds.\n",
    "- Compare two models: a neural network with many layers and many neurons and a neural network with a single neuron. Which of these models is more fitting for this task?\n",
    "> Any of them. Dataset is linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circle dataset\n",
    "- Assume we have access to only one neuron. How many (and what) input features do we need to achieve test loss lower than $0.001$?\n",
    "> To do that we need two input features, but every feature ($x_1$, $x_2$), need to be transform via square transformation.\n",
    "> <img src=\"circle.png\" width=\"800\" height=\"400\">\n",
    "- Assume we have access to only unmodified features (i.e. $x_1$ and $x_2$). Create the smallest (in the number of neurons) neural network which achieves test loss lower than $0.001$. Describe the architecture of the network (including activation functions).\n",
    "> On basis of my try and error, there is needed only 4 neurons in 1 hidden layer with relu activation function and no regularization.\n",
    "> <img src=\"circle2.png\" width=\"800\" height=\"400\"><br>\n",
    "> That same network with 3 neurons and with L2 regularization needed 29 epochs to achieve 0.001 loss.\n",
    "> <img src=\"29epochs.png\" width=\"800\" height=\"400\">\n",
    "- Try to solve the problem with an arbitrary amount of neurons with linear activations (without changing the input features). Did you manage to achieve test loss lower than $0.001$? If yes, describe the architecture. If not, propose a hypothesis as to why it didn't work. \n",
    "> Any number of layers with linear activation function can be replaced with one layer with that activation function. Circle dataset is not linearly separable, thus there is no way to achieve test loss lowen than $0.001$ in that way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spiral dataset\n",
    "\n",
    "- Achieve (stable) test loss lower than $0.1$. Describe the architecture (including the activation function, regularization, and learning rate).\n",
    "> Network contains 1 hidden layers with 8 units, Tanh activation function, 0.01 lr, and without regularization.\n",
    "> <img src=\"spiral2.png\" width=\"800\" height=\"400\">\n",
    "- Which features improve your configuration the most?\n",
    "> $x_1$, $x_2$ (notion of twist, I suppose), $x_1^2$, $x_2^2$ (notion of distance from (0,0), I suppose)\n",
    "- What visually distinguishes solutions which generalize well from solutions which overfit (look at the model visualization after training)?\n",
    "> I could not find model that overfits on spiral dataset. I found only such that generalizes or flatten curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
